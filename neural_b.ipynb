{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_b.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fi7fuKB7_sYM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DdpCFG0-oIwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba11730f-423f-4b1e-86a3-963b21a5a223"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function methods are ready, which will be used in the neural network, if act_type = 0 we use log sigmoid activation function, if act_type = 1 we use tanh activation function, and last if act_type = 2 we use relu activation function.\n",
        "\n",
        "Refer to 'Table of activation functions' from https://en.wikipedia.org/wiki/Activation_function for activation function."
      ],
      "metadata": {
        "id": "aGjF_C3U9VGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will be used as activation function for hidden as well as output layer\n",
        "def tanh(X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "def derivative_tanh(X):\n",
        "    return 1-np.power(X, 2)"
      ],
      "metadata": {
        "id": "Ae9Z7M-1AEWF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function if loss_type = 0 we use Cross Entropy as our Loss function and if loss_type = 1 we use Mean Squared Error as our Loss function. For more information check https://en.wikipedia.org/wiki/Loss_function."
      ],
      "metadata": {
        "id": "GovIrL3WB5Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(Y_train, Y):\n",
        "    multiplier = 1/(2*Y_train.shape[0])\n",
        "    return multiplier*np.sum(np.sum(np.square(np.subtract(Y_train, Y)), axis=0), axis=0)"
      ],
      "metadata": {
        "id": "Nvr8mUsTEXCI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Training Dataset and applying required modifications on training dataset"
      ],
      "metadata": {
        "id": "qvJMuaQIX7RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Devnagri Dataset/train_data_shuffled.csv', header=None)\n",
        "\n",
        "# Last column of train dataset contains Y values\n",
        "Y_train = df_train[df_train.columns[-1]].to_numpy()\n",
        "# Using One hot encoding\n",
        "Y_train = pd.get_dummies(Y_train).to_numpy()\n",
        "\n",
        "print(pd.DataFrame(Y_train))\n",
        "\n",
        "# Dropping Last column of dataframe as Y value has been retained\n",
        "df_train.drop(df_train.columns[-1], inplace=True, axis=1)\n",
        "X_train = df_train.to_numpy()\n",
        "\n",
        "# Scaling pixel values between 0 to 1\n",
        "X_train = X_train/255\n",
        "\n",
        "print(pd.DataFrame(X_train))"
      ],
      "metadata": {
        "id": "asaOlpUqYJzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading testing feature vector and also labels of testing dataset seperately, also making modifications required."
      ],
      "metadata": {
        "id": "X4xWtO4anooa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Devnagri Dataset/public_test.csv', header = None)\n",
        "\n",
        "# Last column contains Labels\n",
        "Y_test = df_test[df_test.columns[-1]].to_numpy()\n",
        "print(pd.DataFrame(Y_test))\n",
        "\n",
        "# Dropping last column to get X_test \n",
        "df_test.drop(df_test.columns[-1], inplace=True, axis=1)\n",
        "X_test = df_test.to_numpy()\n",
        "# Scaling X_test from 0 to 1\n",
        "X_test = X_test/255\n",
        "X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "\n",
        "print(pd.DataFrame(X_test))"
      ],
      "metadata": {
        "id": "Ujwe12Jdneli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Setting Neural Network Parameters like number of epochs, batch_size, neurons in each layer(list is passed), learning rate type(for normal gradient descent, adaptive gradient descent), learning rate, activation type(sigmoid activation, tanh activation, RELU activation, Softmax Activation), loss function type(Cross Entropy loss Function, Mean Squared Error loss function), initial seed value(Taking default value as 87)."
      ],
      "metadata": {
        "id": "IDjjQfJ_nkkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to open params.txt here instead of manual data entering\n",
        "epochs = 100\n",
        "batch_size = 1700\n",
        "layers = [512,256,128,46]\n",
        "lr_type = 1\n",
        "lr = 2\n",
        "act_type = 1\n",
        "loss_type = 1\n",
        "seed_val = 22"
      ],
      "metadata": {
        "id": "1hCIzv3LnmvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing random initialisation of neural nets weights matrix according to Xavier's Initialisation of weights."
      ],
      "metadata": {
        "id": "aQelHk51osAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training datavalues\n",
        "N = X_train.shape[0]\n",
        "\n",
        "weights = []\n",
        "\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# Initial number of neurons which is feature of dataset then after each iteration this will change to previous layers neurons\n",
        "m = X_train.shape[1]\n",
        "\n",
        "# Initialising weights\n",
        "for i in range(len(layers)):\n",
        "    n = layers[i]\n",
        "\n",
        "    # Initialisation of weights of each layer,\n",
        "    # Random data values from a normal distribution whose mean is 0 \n",
        "    # and std. deviation is 1 and output dimension is (m+1 cross n) [previous_layer_neurons+1  cross current_layer_neurons] \n",
        "    weights.append(np.float64(np.random.normal(0,1,(m+1,n))*np.sqrt(2/(m+n+1))))\n",
        "\n",
        "    # change the number of input neurons\n",
        "    m = n\n",
        "\n",
        "# Prints weight matrix of last layer\n",
        "for i in range(len(layers)):\n",
        "    print(weights[i].shape)"
      ],
      "metadata": {
        "id": "P2lfcUsXo0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing Training Dataset into batches of batch_size."
      ],
      "metadata": {
        "id": "8_TfhRGKpU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing data into batches\n",
        "training_batch = []\n",
        "# Adding a columns of ones to absorb bias in the training data\n",
        "print(pd.DataFrame(X_train))\n",
        "print(\"======================================================\")\n",
        "X_train = np.concatenate((np.ones((N,1)), X_train), axis=1)\n",
        "print(pd.DataFrame(X_train))\n",
        "\n",
        "number_batches = N//batch_size\n",
        "for i in range(number_batches):\n",
        "    # Adding a tupple of (X,Y) in batches\n",
        "    training_batch.append((X_train[i*batch_size:(i+1)*batch_size,:], Y_train[i*batch_size:(i+1)*batch_size,:]))\n"
      ],
      "metadata": {
        "id": "V4PZMPk7pkl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Training Our Neural Network According to given training Dataset"
      ],
      "metadata": {
        "id": "nXetEuJJs8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making loss array to store loss value after each epoch to compare loss values according to epoch\n",
        "tmp = lr\n",
        "loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # For adaptive gradient descent\n",
        "    if lr_type == 1:\n",
        "        lr = tmp/math.sqrt(epoch+1)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # Go over every batch and train my neural network\n",
        "    for batch, Y in training_batch:\n",
        "        values = []\n",
        "        values.append(batch)\n",
        "\n",
        "        # store original batch as this will get modified in next for loop\n",
        "        batch1 = batch\n",
        "        # Forward Propogation this batch so that it can be used in Backpropogation\n",
        "        for i in range(len(layers)-1):\n",
        "            batch = tanh(np.dot(batch, weights[i]))\n",
        "            \n",
        "            # Concatenate with a columns of ones after output from each layer to absorb bias in the data\n",
        "            batch = np.concatenate((np.ones((batch_size, 1)),batch), axis = 1)\n",
        "            # caching this in values so that it can be used in backpropogation\n",
        "            values.append(batch)\n",
        "        \n",
        "        # forward propogation for last layer\n",
        "        output = tanh(np.dot(batch, weights[i+1]))\n",
        "\n",
        "        # Final output of this neural network for this batch without tweeking weights wrong output most probably\n",
        "        values.append(output)\n",
        "\n",
        "        # Calculating derivative of loss function for backpropogation of neural net\n",
        "        der = (output-Y)*derivative_tanh(output)/Y.shape[0]\n",
        "        \n",
        "        # Backpropogating the error \n",
        "        for i in range(len(layers)-1,-1,-1):\n",
        "            weight_copy = weights[i].copy()\n",
        "            w = np.dot(values[i].T, der)\n",
        "            weights[i] -= lr*w\n",
        "            der = np.dot(der, weight_copy.T)*derivative_tanh(values[i])                        \n",
        "            der = np.delete(der, 0, axis = 1)\n",
        "\n",
        "        # Calculating predicted output of this batch to calculate loss value of this batch\n",
        "        output = batch1\n",
        "        for i in range(len(layers)-1):\n",
        "            output = np.dot(output, weights[i])\n",
        "            output = tanh(output)\n",
        "            output = np.concatenate((np.ones((output.shape[0], 1)), output), axis = 1)\n",
        "        \n",
        "        # Forward propogating for output layer\n",
        "        output = tanh(np.dot(output, weights[i+1]))\n",
        "\n",
        "        # According to loss type calculate loss value of this batch\n",
        "        total_loss += MSE(Y, output)\n",
        "        \n",
        "    # append to this epoch's loss value\n",
        "    loss.append(total_loss)\n",
        "\n",
        "    print(\"Training Loss on \" + str(epoch+1) +\" iteration --> \" + str(total_loss))\n",
        "\n",
        "print(\"Graphical representation of Training Loss vs number of epochs -->\")\n",
        "plt.plot([(epoch+1) for epoch in range(epochs)], loss)\n",
        "plt.show()       "
      ],
      "metadata": {
        "id": "axQg8rkGtEPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now predicting the model on testing dataset, as you will see that it is highly inaccurate and it needs 500 epochs to show good results. "
      ],
      "metadata": {
        "id": "9LDP1OEJ5WS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(layers)-1):\n",
        "    X_test = tanh(np.dot(X_test, weights[i]))\n",
        "    X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "\n",
        "X_test = tanh(np.dot(X_test, weights[i+1]))\n",
        "pred = np.argmax(X_test, axis=1)\n",
        "\n",
        "correct = 0\n",
        "total = pred.shape[0]\n",
        "\n",
        "for i in range(total):\n",
        "    if pred[i] == Y_test[i]:\n",
        "        correct+=1\n",
        "\n",
        "print(\"Accuracy On testing dataset ---> \" +str((correct/total)*100)) "
      ],
      "metadata": {
        "id": "Y3kGzuPa5jGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}