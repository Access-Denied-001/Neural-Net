{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fi7fuKB7_sYM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdpCFG0-oIwy",
        "outputId": "085c4430-9ac4-4d4c-8e04-ae833e9e24ad"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function methods are ready, which will be used in the neural network, if act_type = 0 we use log sigmoid activation function, if act_type = 1 we use tanh activation function, and last if act_type = 2 we use relu activation function.\n",
        "\n",
        "Refer to 'Table of activation functions' from https://en.wikipedia.org/wiki/Activation_function for activation function."
      ],
      "metadata": {
        "id": "aGjF_C3U9VGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "    return (1/(1+np.exp(-X)))\n",
        "\n",
        "# Will be used as activation function for hidden as well as output layer\n",
        "def tanh(X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "def relu(X):\n",
        "    return X.clip(0)\n",
        "\n",
        "def derivative_sigmoid(X):\n",
        "    return X*(1-X)\n",
        "\n",
        "def derivative_tanh(X):\n",
        "    return 1-np.power(X, 2)\n",
        "\n",
        "def derivative_relu(X):\n",
        "    return (X>0).astype('float32')\n",
        "\n",
        "# Will be used in last Layer if Cross Entropy losss function is used\n",
        "def softmax(X):\n",
        "    x = np.exp(X)\n",
        "    return x/np.sum(x, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "Ae9Z7M-1AEWF"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function if loss_type = 0 we use Cross Entropy as our Loss function and if loss_type = 1 we use Mean Squared Error as our Loss function. For more information check https://en.wikipedia.org/wiki/Loss_function."
      ],
      "metadata": {
        "id": "GovIrL3WB5Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CE(Y_train, Y):\n",
        "    multiplier = -1/Y_train.shape[0]\n",
        "    eps = 1e-15\n",
        "    return multiplier*np.sum(np.log(np.clip(np.sum(Y_train*Y, axis=1), eps, 1-eps)))\n",
        "    \n",
        "def MSE(Y_train, Y):\n",
        "    multiplier = 1/(2*Y_train.shape[0])\n",
        "    return multiplier*np.sum(np.sum(np.square(np.subtract(Y_train, Y)), axis=0), axis=0)"
      ],
      "metadata": {
        "id": "Nvr8mUsTEXCI"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Training Dataset and applying required modifications on training dataset"
      ],
      "metadata": {
        "id": "qvJMuaQIX7RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_train.csv', header=None)\n",
        "\n",
        "# First column of train dataset contains Y values\n",
        "Y_train = df_train[df_train.columns[0]].to_numpy()\n",
        "Y_train = pd.get_dummies(Y_train).to_numpy()\n",
        "\n",
        "# Dropping first column of dataframe as Y value has been retained\n",
        "df_train.drop(df_train.columns[0], inplace=True, axis=1)\n",
        "X_train = df_train.to_numpy()\n",
        "# Scaling pixel values between 0 to 1\n",
        "X_train = X_train/255\n",
        "\n",
        "print(pd.DataFrame(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asaOlpUqYJzK",
        "outputId": "e3d07ae5-390c-4609-a69c-6699ff99de52"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0         1         2         3    ...       196       197       198       199\n",
            "0     0.0  0.000000  0.000000  0.000000  ...  0.941176  0.278431  0.074510  0.109804\n",
            "1     0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.125490\n",
            "2     0.0  0.125490  0.929412  0.992157  ...  0.000000  0.000000  0.000000  1.000000\n",
            "3     0.0  0.000000  0.019608  0.247059  ...  0.000000  0.000000  0.000000  0.698039\n",
            "4     0.0  0.003922  0.658824  0.949020  ...  0.000000  0.000000  0.113725  0.996078\n",
            "...   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
            "2995  0.0  0.039216  0.650980  0.882353  ...  0.403922  0.000000  0.000000  0.000000\n",
            "2996  0.0  0.321569  0.807843  0.992157  ...  0.000000  0.000000  0.000000  0.000000\n",
            "2997  0.0  0.000000  0.000000  0.031373  ...  0.898039  0.270588  0.058824  0.000000\n",
            "2998  0.0  0.000000  0.000000  0.529412  ...  0.000000  0.000000  0.000000  1.000000\n",
            "2999  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
            "\n",
            "[3000 rows x 200 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading testing feature vector and also labels of testing dataset seperately, also making modifications required."
      ],
      "metadata": {
        "id": "X4xWtO4anooa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First colum of test dataset contains -1s so we need to drop the first column\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_test.csv', header=None)\n",
        "X_test = df_test.to_numpy()\n",
        "X_test = X_test[:,1:]\n",
        "X_test = X_test/255\n",
        "\n",
        "df_test1 = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_test_labels.csv', header=None)\n",
        "Y_test = df_test1.to_numpy()\n",
        "\n",
        "print(pd.DataFrame(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujwe12Jdneli",
        "outputId": "52877de6-3d0c-411c-a935-59edad4e3a06"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0         1         2         3    ...       196       197       198       199\n",
            "0    0.0  0.000000  0.000000  0.035294  ...  0.000000  0.003922  0.435294  0.992157\n",
            "1    0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.945098\n",
            "2    0.0  0.156863  0.898039  0.992157  ...  0.000000  0.000000  0.819608  0.992157\n",
            "3    0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.854902\n",
            "4    0.0  0.000000  0.000000  0.000000  ...  0.992157  0.992157  0.815686  0.933333\n",
            "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
            "295  0.0  0.000000  0.000000  0.000000  ...  0.992157  0.568627  0.003922  0.313725\n",
            "296  0.0  0.000000  0.000000  0.019608  ...  0.243137  0.000000  0.000000  0.000000\n",
            "297  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.443137\n",
            "298  0.0  0.000000  0.160784  0.517647  ...  0.000000  0.000000  0.000000  0.000000\n",
            "299  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.141176\n",
            "\n",
            "[300 rows x 200 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Setting Neural Network Parameters like number of epochs, batch_size, neurons in each layer(list is passed), learning rate type(for normal gradient descent, adaptive gradient descent), learning rate, activation type(sigmoid activation, tanh activation, RELU activation, Softmax Activation), loss function type(Cross Entropy loss Function, Mean Squared Error loss function), initial seed value(Taking default value as 87)."
      ],
      "metadata": {
        "id": "IDjjQfJ_nkkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to open params.txt here instead of manual data entering\n",
        "epochs = 500\n",
        "batch_size = 100\n",
        "layers = [100,50,20,2]\n",
        "lr_type = 1\n",
        "lr = 2\n",
        "act_type = 0\n",
        "loss_type = 0\n",
        "seed_val = 22"
      ],
      "metadata": {
        "id": "1hCIzv3LnmvT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing random initialisation of neural nets weights matrix according to Xavier's Initialisation of weights."
      ],
      "metadata": {
        "id": "aQelHk51osAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training datavalues\n",
        "N = X_train.shape[0]\n",
        "\n",
        "weights = []\n",
        "\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# Initial number of neurons which is feature of dataset then after each iteration this will change to previous layers neurons\n",
        "m = X_train.shape[1]\n",
        "\n",
        "# Initialising weights\n",
        "for i in range(len(layers)):\n",
        "    n = layers[i]\n",
        "\n",
        "    # Initialisation of weights of each layer,\n",
        "    # Random data values from a normal distribution whose mean is 0 \n",
        "    # and std. deviation is 1 and output dimension is (m+1 cross n) [previous_layer_neurons+1  cross current_layer_neurons] \n",
        "    weights.append(np.float64(np.random.normal(0,1,(m+1,n))*np.sqrt(2/(m+n+1))))\n",
        "\n",
        "    # change the number of input neurons\n",
        "    m = n\n",
        "\n",
        "# Prints weight matrix of last layer\n",
        "for i in range(len(layers)):\n",
        "    print(weights[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2lfcUsXo0TA",
        "outputId": "b6c843ec-be56-4873-ae24-aaef6ab25c17"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(201, 100)\n",
            "(101, 50)\n",
            "(51, 20)\n",
            "(21, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing Training Dataset into batches of batch_size."
      ],
      "metadata": {
        "id": "8_TfhRGKpU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing data into batches\n",
        "training_batch = []\n",
        "# Adding a columns of ones to absorb bias in the training data\n",
        "print(pd.DataFrame(X_train))\n",
        "print(\"======================================================\")\n",
        "X_train = np.concatenate((np.ones((X_train.shape[0],1)), X_train), axis=1)\n",
        "print(pd.DataFrame(X_train))\n",
        "\n",
        "number_batches = N//batch_size\n",
        "for i in range(number_batches):\n",
        "    # Adding a tupple of (X,Y) in batches\n",
        "    training_batch.append((X_train[i*batch_size:(i+1)*batch_size,:], Y_train[i*batch_size:(i+1)*batch_size,:]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4PZMPk7pkl2",
        "outputId": "e66af146-68ca-4034-d851-e121ca9e084f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0         1         2         3    ...       196       197       198       199\n",
            "0     0.0  0.000000  0.000000  0.000000  ...  0.941176  0.278431  0.074510  0.109804\n",
            "1     0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.125490\n",
            "2     0.0  0.125490  0.929412  0.992157  ...  0.000000  0.000000  0.000000  1.000000\n",
            "3     0.0  0.000000  0.019608  0.247059  ...  0.000000  0.000000  0.000000  0.698039\n",
            "4     0.0  0.003922  0.658824  0.949020  ...  0.000000  0.000000  0.113725  0.996078\n",
            "...   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
            "2995  0.0  0.039216  0.650980  0.882353  ...  0.403922  0.000000  0.000000  0.000000\n",
            "2996  0.0  0.321569  0.807843  0.992157  ...  0.000000  0.000000  0.000000  0.000000\n",
            "2997  0.0  0.000000  0.000000  0.031373  ...  0.898039  0.270588  0.058824  0.000000\n",
            "2998  0.0  0.000000  0.000000  0.529412  ...  0.000000  0.000000  0.000000  1.000000\n",
            "2999  0.0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
            "\n",
            "[3000 rows x 200 columns]\n",
            "======================================================\n",
            "      0    1         2         3    ...       197       198       199       200\n",
            "0     1.0  0.0  0.000000  0.000000  ...  0.941176  0.278431  0.074510  0.109804\n",
            "1     1.0  0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.125490\n",
            "2     1.0  0.0  0.125490  0.929412  ...  0.000000  0.000000  0.000000  1.000000\n",
            "3     1.0  0.0  0.000000  0.019608  ...  0.000000  0.000000  0.000000  0.698039\n",
            "4     1.0  0.0  0.003922  0.658824  ...  0.000000  0.000000  0.113725  0.996078\n",
            "...   ...  ...       ...       ...  ...       ...       ...       ...       ...\n",
            "2995  1.0  0.0  0.039216  0.650980  ...  0.403922  0.000000  0.000000  0.000000\n",
            "2996  1.0  0.0  0.321569  0.807843  ...  0.000000  0.000000  0.000000  0.000000\n",
            "2997  1.0  0.0  0.000000  0.000000  ...  0.898039  0.270588  0.058824  0.000000\n",
            "2998  1.0  0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  1.000000\n",
            "2999  1.0  0.0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
            "\n",
            "[3000 rows x 201 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Training Our Neural Network According to given training Dataset"
      ],
      "metadata": {
        "id": "nXetEuJJs8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making loss array to store loss value after each epoch to compare loss values according to epoch\n",
        "tmp = lr\n",
        "loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # For adaptive gradient descent\n",
        "    if lr_type == 1:\n",
        "        lr = tmp/math.sqrt(epoch+1)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # Go over every batch and train my neural network\n",
        "    for batch, Y in training_batch:\n",
        "        values = []\n",
        "        values.append(batch)\n",
        "\n",
        "        # store original batch as this will get modified in next for loop\n",
        "        batch1 = batch\n",
        "        # Forward Propogation this batch so that it can be used in Backpropogation\n",
        "        for i in range(len(layers)-1):\n",
        "            batch = np.dot(batch, weights[i])\n",
        "            if act_type == 0:\n",
        "                batch = sigmoid(batch)\n",
        "            elif act_type == 1:\n",
        "                batch = tanh(batch)\n",
        "            else:\n",
        "                batch = relu(batch)\n",
        "            \n",
        "            # Concatenate with a columns of ones after output from each layer to absorb bias in the data\n",
        "            batch = np.concatenate((np.ones((batch_size, 1)),batch), axis = 1)\n",
        "\n",
        "            # catching this in values so that it can be used in backpropogation\n",
        "            values.append(batch)\n",
        "        \n",
        "        # forward propogation for last layer\n",
        "        batch = np.dot(batch, weights[i+1])\n",
        "\n",
        "        # Activation function for last layer\n",
        "        if loss_type == 0:\n",
        "            output = softmax(batch)\n",
        "        else:\n",
        "            output = tanh(batch_size)\n",
        "\n",
        "        # Final output of this neural network for this batch without tweeking weights wrong output most probably\n",
        "        values.append(output)\n",
        "\n",
        "        # Calculating derivative of loss function for backpropogation of neural net\n",
        "        if loss_type == 0:\n",
        "            der = (output-Y)/Y.shape[0]\n",
        "\n",
        "        else:\n",
        "            if act_type == 0:\n",
        "                der = (output-Y)*derivative_sigmoid(output)/Y.shape[0]\n",
        "            elif act_type:\n",
        "                der = (output-Y)*derivative_tanh(output)/Y.shape[0]\n",
        "            else:\n",
        "                der = (output-Y)*derivative_relu(output)/Y.shape[0]\n",
        "        \n",
        "        # Back propogating error \n",
        "        for i in range(len(layers)-1,-1,-1):\n",
        "            weight_copy = weights[i].copy()\n",
        "            w = np.dot(values[i].T, der)\n",
        "\n",
        "            weights[i] -= lr*w\n",
        "\n",
        "            if act_type == 0:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_sigmoid(values[i])\n",
        "            elif act_type == 1:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_tanh(values[i])\n",
        "            else:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_relu(values[i])\n",
        "            \n",
        "            der = np.delete(der, 0, axis = 1)\n",
        "\n",
        "        # Calculating predicted output of this batch to calculate loss value of this batch\n",
        "        output = batch1\n",
        "        for i in range(len(layers)-1):\n",
        "            output = np.dot(output, weights[i])\n",
        "            if act_type == 0:\n",
        "                output = sigmoid(output)\n",
        "            elif act_type == 1:\n",
        "                output = tanh(output)\n",
        "            else:\n",
        "                output = relu(output)\n",
        "\n",
        "            output = np.concatenate((np.ones((output.shape[0], 1)), output), axis = 1)\n",
        "        \n",
        "        # Forward propogating for output layer\n",
        "        output = np.dot(output, weights[i+1])\n",
        "        # Activation function for last layer\n",
        "        if loss_type == 0:\n",
        "            output = softmax(output)\n",
        "        else:\n",
        "            output = tanh(output)\n",
        "\n",
        "        # According to loss type calculate loss value of this batch\n",
        "        if loss_type == 0:\n",
        "            total_loss += CE(Y, output)\n",
        "        else:\n",
        "            total_loss += MSE(Y, output)\n",
        "    # append to this epoch's loss value\n",
        "    loss.append(total_loss)\n",
        "\n",
        "# printing training loss\n",
        "print(loss)\n",
        "        \n",
        "# Predicting output for testing dataset\n",
        "for i in range(len(layers)-1):\n",
        "    X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "    X_test = np.dot(X_test, weights[i])\n",
        "    \n",
        "    if act_type == 0:\n",
        "        X_test = sigmoid(X_test)\n",
        "    elif act_type == 1:\n",
        "        X_test = tanh(X_test)\n",
        "    else:\n",
        "        X_test = relu(X_test)\n",
        "\n",
        "X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "X_test = np.dot(X_test, weights[i+1])\n",
        "\n",
        "if loss_type == 0:\n",
        "    output = softmax(X_test)\n",
        "else:\n",
        "    output = tanh(X_test)\n",
        "pred = np.argmax(output, axis=1)\n",
        "\n",
        "correct = 0\n",
        "total = pred.shape[0]\n",
        "\n",
        "for i in range(total):\n",
        "    if pred[i] == Y_test[i]:\n",
        "        correct+=1\n",
        "\n",
        "print((correct/total)*100)        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axQg8rkGtEPY",
        "outputId": "5adf862d-1e77-4cef-f920-67957615db3b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23.983077631797943, 12.743962524115155, 1.5343864601358028, 1.0169486294745975, 0.8631695300955182, 0.784682881869288, 0.7348749130362808, 0.6988951335212534, 0.670719077840736, 0.6474773160823953, 0.6276172518121114, 0.610210227162572, 0.5946608965737276, 0.5805712313655067, 0.5676665132916316, 0.5557489496520558, 0.5446671682576543, 0.5342973118672868, 0.5245329615333727, 0.5152809720354801, 0.5064603591557264, 0.4980020914178268, 0.48984866243103586, 0.48195316091708074, 0.47427799187922737, 0.46679350158770305, 0.4594766934797898, 0.4523101240046242, 0.44528099428620443, 0.4383804136309397, 0.43160279487508507, 0.4249453387214417, 0.4184075678043061, 0.4119908782252346, 0.4056980859517297, 0.3995329577748215, 0.3934997310304882, 0.38760264122521654, 0.38184548875641133, 0.37623128105578635, 0.3707619817543309, 0.36543838396477, 0.36026010447325724, 0.3552256761223795, 0.35033270324441596, 0.3455780427749992, 0.3409579805158876, 0.33646838378706273, 0.33210482368341876, 0.3278626690222099, 0.3237371586779485, 0.3197234599191272, 0.3158167189642958, 0.31201210765912873, 0.30830486793448886, 0.30469035404777217, 0.3011640716667857, 0.2977217125342857, 0.29435918356705065, 0.29107262960299507, 0.2878584494551173, 0.28471330535278516, 0.28163412618699263, 0.27861810520290314, 0.2756626929030322, 0.27276558595648753, 0.26992471287940506, 0.26713821718526487, 0.264404438623737, 0.26172189305009236, 0.25908925140473205, 0.2565053182388514, 0.2539690101974197, 0.25147933486044355, 0.2490353703411431, 0.24663624603720205, 0.24428112492057844, 0.24196918772556067, 0.23969961934909834, 0.23747159771014348, 0.23528428522734834, 0.23313682297175248, 0.23102832744050478, 0.2289578897882287, 0.22692457725381018, 0.22492743644052748, 0.2229654980528327, 0.22103778266701107, 0.2191433071154778, 0.21728109109272795, 0.215450163639705, 0.21364956922602552, 0.21187837321907205, 0.21013566659896418, 0.208420569843478, 0.2067322359632688, 0.20506985271304976, 0.2034326440379099, 0.20181987083618325, 0.20023083113245074, 0.1986648597581434, 0.1971213276347668, 0.19559964074784952, 0.1940992388900473, 0.192619594240791, 0.19116020983852086, 0.18972061799069254, 0.18830037865684637, 0.1868990778314144, 0.1855163259456679, 0.18415175630232386, 0.18280502355168993, 0.18147580221473564, 0.18016378525595417, 0.17886868270715411, 0.17759022034225602, 0.176328138402587, 0.17508219037197476, 0.17385214180099626, 0.17263776917997, 0.17143885886059937, 0.17025520602653257, 0.1690866137134449, 0.16793289187954002, 0.1667938565275925, 0.16566932887977343, 0.1645591346065505, 0.1634631031108688, 0.16238106686867482, 0.16131286082658397, 0.16025832185717184, 0.15921728827197748, 0.1581895993918748, 0.15717509517399658, 0.1561736158939261, 0.15518500188138756, 0.1542090933072239, 0.15324573001901995, 0.15229475142236865, 0.15135599640445102, 0.1504293032963558, 0.14951450987037804, 0.14861145336842085, 0.1477199705575845, 0.14683989780904252, 0.14597107119639713, 0.14511332660983728, 0.14426649988261606, 0.14343042692658817, 0.14260494387380623, 0.14178988722145341, 0.14098509397768313, 0.14019040180623668, 0.13940564916800363, 0.13863067545798202, 0.13786532113637193, 0.13710942785279528, 0.13636283856287587, 0.1356253976366306, 0.13489695095832074, 0.13417734601757952, 0.13346643199178282, 0.1327640598197546, 0.13207008226700145, 0.13138435398275483, 0.1307067315491652, 0.13003707352304086, 0.12937524047055965, 0.1287210949954017, 0.1280745017607668, 0.12743532750573755, 0.126803441056447, 0.12617871333250105, 0.12556101734908243, 0.12495022821515506, 0.12434622312815488, 0.12374888136554095, 0.12315808427354684, 0.1225737152534546, 0.12199565974568788, 0.12142380521199406, 0.1208580411159673, 0.12029825890213841, 0.11974435197383809, 0.1191962156700222, 0.11865374724122438, 0.1181168458247932, 0.11758541241954402, 0.11705934985995127, 0.11653856278998878, 0.11602295763671304, 0.11551244258367738, 0.1150069275442502, 0.11450632413490465, 0.11401054564853927, 0.11351950702787837, 0.11303312483899969, 0.11255131724502412, 0.11207400398000557, 0.11160110632304501, 0.11113254707265781, 0.11066825052141058, 0.1102081424308487, 0.10975215000672787, 0.10930020187456108, 0.10885222805549004, 0.1084081599424921, 0.1079679302769237, 0.10753147312540905, 0.1070987238570724, 0.10666961912112073, 0.10624409682477177, 0.10582209611153218, 0.1054035573398201, 0.10498842206193423, 0.1045766330033632, 0.10416813404243472, 0.10376287019030027, 0.10336078757124992, 0.10296183340335664, 0.10256595597943992, 0.1021731046483501, 0.101783229796563, 0.10139628283008388, 0.10101221615665193, 0.10063098316824265, 0.10025253822386082, 0.09987683663261897, 0.09950383463709628, 0.09913348939697109, 0.09876575897292278, 0.09840060231079568, 0.09803797922602164, 0.0976778503882921, 0.0973201773064785, 0.096964922313792, 0.09661204855317798, 0.0962615199629407, 0.0959133012625916, 0.0955673579389163, 0.09522365623225623, 0.09488216312299651, 0.09454284631825928, 0.09420567423879388, 0.09387061600606085, 0.09353764142950427, 0.0932067209940079, 0.09287782584753049, 0.09255092778891592, 0.09222599925587274, 0.09190301331312069, 0.09158194364069816, 0.09126276452242738, 0.09094545083453374, 0.09062997803441428, 0.09031632214955294, 0.09000445976657777, 0.08969436802045762, 0.08938602458383488, 0.08907940765648964, 0.08877449595493483, 0.08847126870213712, 0.08816970561736134, 0.08786978690613693, 0.08757149325034252, 0.08727480579840632, 0.08697970615562174, 0.08668617637457357, 0.08639419894567504, 0.08610375678781274, 0.0858148332390976, 0.08552741204772167, 0.08524147736291748, 0.08495701372601988, 0.08467400606162954, 0.08439243966887519, 0.08411230021277635, 0.08383357371570371, 0.08355624654893706, 0.08328030542432098, 0.0830057373860171, 0.0827325298023526, 0.08246067035776519, 0.08219014704484438, 0.08192094815646835, 0.08165306227803822, 0.08138647827980777, 0.08112118530931008, 0.0808571727838818, 0.0805944303832843, 0.08033294804242314, 0.08007271594416625, 0.07981372451226057, 0.07955596440434909, 0.07929942650508832, 0.07904410191936671, 0.07878998196562512, 0.07853705816927978, 0.07828532225624991, 0.07803476614658755, 0.07778538194821474, 0.07753716195076539, 0.07729009861953423, 0.07704418458953304, 0.07679941265965586, 0.07655577578695258, 0.07631326708101237, 0.07607187979845773, 0.07583160733754957, 0.07559244323290391, 0.07535438115032043, 0.07511741488172417, 0.07488153834021964, 0.07464674555525906, 0.07441303066792357, 0.07418038792631873, 0.07394881168108362, 0.07371829638101507, 0.07348883656880384, 0.07326042687688705, 0.07303306202341317, 0.07280673680831981, 0.07258144610952666, 0.07235718487923866, 0.07213394814036314, 0.07191173098303742, 0.0716905285612681, 0.071470336089679, 0.07125114884036975, 0.07103296213988126, 0.07081577136626932, 0.0705995719462835, 0.07038435935265094, 0.0701701291014639, 0.0699568767496693, 0.06974459789265872, 0.06953328816195804, 0.06932294322301497, 0.06911355877308109, 0.06890513053919048, 0.06869765427622897, 0.06849112576509525, 0.0682855408109511, 0.06808089524155758, 0.06787718490569758, 0.06767440567168119, 0.06747255342593163, 0.06727162407165102, 0.06707161352756359, 0.06687251772673265, 0.06667433261545183, 0.0664770541522063, 0.06628067830670323, 0.06608520105896848, 0.06589061839850793, 0.06569692632353129, 0.06550412084023588, 0.06531219796214867, 0.06512115370952438, 0.06493098410879743, 0.06474168519208554, 0.06455325299674379, 0.06436568356496575, 0.06417897294343038, 0.0639931171829937, 0.06380811233842097, 0.06362395446815941, 0.06344063963414942, 0.06325816390167116, 0.06307652333922567, 0.06289571401844925, 0.06271573201405792, 0.0625365734038213, 0.06235823426856415, 0.0621807106921939, 0.06200399876175177, 0.06182809456748771, 0.061652994202956245, 0.06147869376513172, 0.06130518935454287, 0.06113247707542412, 0.06096055303588263, 0.06078941334808062, 0.06061905412843011, 0.060449471497800666, 0.060280661581737865, 0.0601126205106921, 0.059945344420256653, 0.059778829451412406, 0.05961307175078213, 0.05944806747088796, 0.059283812770417166, 0.05912030381449014, 0.05895753677493328, 0.05879550783055492, 0.05863421316742271, 0.05847364897914368, 0.05831381146714446, 0.05815469684095241, 0.057996301318476405, 0.05783862112628679, 0.05768165249989483, 0.05752539168402952, 0.05736983493291293, 0.057214978510533115, 0.05706081869091399, 0.056907351758382047, 0.05675457400782977, 0.056602481744974384, 0.056451071286613676, 0.05630033896087688, 0.05615028110747086, 0.0560008940779218, 0.055852174235811514, 0.05570411795700928, 0.05555672162989768, 0.05540998165559337, 0.05526389444816252, 0.05511845643482972, 0.05497366405618263, 0.05482951376636919, 0.05468600203328991, 0.054543125338784575, 0.05440088017881182, 0.05425926306362375, 0.0541182705179344, 0.05397789908108166, 0.05383814530718409, 0.053699005765290846, 0.0535604770395262, 0.05342255572922888, 0.053285238449083255, 0.05314852182924738, 0.05301240251547352, 0.05287687716922379, 0.05274194246777928, 0.05260759510434498, 0.05247383178814739, 0.05234064924452834, 0.05220804421503222, 0.05207601345748888, 0.05194455374609019, 0.05181366187146279, 0.05168333464073444, 0.05155356887759693, 0.051424361422362445, 0.051295709132016386, 0.05116760888026534, 0.05104005755757979, 0.05091305207123325, 0.050786589345336294, 0.05066066632086688, 0.05053527995569578, 0.05041042722460835, 0.05028610511932262, 0.05016231064850268, 0.050039040837768986, 0.04991629272970433, 0.049794063383857, 0.049672349876739026, 0.04955114930182289, 0.049430458769533475, 0.049310275407236696, 0.049190596359226256, 0.04907141878670621, 0.048952739867770614, 0.048834556797380996, 0.04871686678734033, 0.04859966706626457, 0.0484829548795515, 0.0483667274893472, 0.04825098217450993, 0.04813571623057112, 0.048020926969695066, 0.04790661172063583, 0.04779276782869166, 0.0476793926556578, 0.047566483579777434, 0.04745403799569017, 0.0473420533143789, 0.04723052696311499, 0.04711945638540169, 0.04700883904091546, 0.04689867240544665, 0.04678895397083753, 0.046679681244919675, 0.046570851751449316, 0.046462463030041985, 0.04635451263610559, 0.046246998140771496, 0.04613991713082615, 0.0460332672086397, 0.0459270459920948, 0.04582125111451374, 0.045715880224585045, 0.04561093098628878, 0.045506401078821285, 0.04540228819651865, 0.045298590048779806, 0.04519530435998878]\n",
            "99.0\n"
          ]
        }
      ]
    }
  ]
}