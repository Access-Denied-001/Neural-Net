{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fi7fuKB7_sYM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdpCFG0-oIwy",
        "outputId": "50a931f8-77d8-4901-c99a-9b1f729660e2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function methods are ready, which will be used in the neural network, if act_type = 0 we use log sigmoid activation function, if act_type = 1 we use tanh activation function, and last if act_type = 2 we use relu activation function.\n",
        "\n",
        "Refer to 'Table of activation functions' from https://en.wikipedia.org/wiki/Activation_function for activation function."
      ],
      "metadata": {
        "id": "aGjF_C3U9VGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "    return (1/(1+np.exp(-X)))\n",
        "\n",
        "# Will be used as activation function for hidden as well as output layer\n",
        "def tanh(X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "def relu(X):\n",
        "    return X.clip(0)\n",
        "\n",
        "def derivative_sigmoid(X):\n",
        "    return X*(1-X)\n",
        "\n",
        "def derivative_tanh(X):\n",
        "    return 1-np.power(X, 2)\n",
        "\n",
        "def derivative_relu(X):\n",
        "    return (X>0).astype('float32')\n",
        "\n",
        "# Will be used in last Layer if Cross Entropy losss function is used\n",
        "def softmax(X):\n",
        "    x = np.exp(X)\n",
        "    return x/np.sum(x, axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "Ae9Z7M-1AEWF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function if loss_type = 0 we use Cross Entropy as our Loss function and if loss_type = 1 we use Mean Squared Error as our Loss function. For more information check https://en.wikipedia.org/wiki/Loss_function."
      ],
      "metadata": {
        "id": "GovIrL3WB5Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CE(Y_train, Y):\n",
        "    multiplier = -1/Y_train.shape[0]\n",
        "    eps = 1e-15\n",
        "    return multiplier*np.sum(np.log(np.clip(np.sum(Y_train*Y, axis=1), eps, 1-eps)))\n",
        "    \n",
        "def MSE(Y_train, Y):\n",
        "    multiplier = 1/(2*Y_train.shape[0])\n",
        "    return multiplier*np.sum(np.sum(np.square(np.subtract(Y_train, Y)), axis=0), axis=0)"
      ],
      "metadata": {
        "id": "Nvr8mUsTEXCI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Training Dataset and applying required modifications on training dataset"
      ],
      "metadata": {
        "id": "qvJMuaQIX7RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_train.csv', header=None)\n",
        "\n",
        "# First column of train dataset contains Y values\n",
        "Y_train = df_train[df_train.columns[0]].to_numpy()\n",
        "Y_train = pd.get_dummies(Y_train).to_numpy()\n",
        "\n",
        "# Dropping first column of dataframe as Y value has been retained\n",
        "df_train.drop(df_train.columns[0], inplace=True, axis=1)\n",
        "X_train = df_train.to_numpy()\n",
        "# Scaling pixel values between 0 to 1\n",
        "X_train = X_train/255\n",
        "\n",
        "print(\"Training input data --> 3000 images with 200 cols(features)\")\n",
        "print(pd.DataFrame(X_train))\n",
        "\n",
        "print(\"Training output label --> One hot encoded\")\n",
        "print(pd.DataFrame(Y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asaOlpUqYJzK",
        "outputId": "c579b9b1-dc57-492f-ebfb-1ab972657342"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training input data --> 3000 images with 200 cols(features)\n",
            "      0         1         2         3         4         5         6    \\\n",
            "0     0.0  0.000000  0.000000  0.000000  0.188235  0.933333  0.988235   \n",
            "1     0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2     0.0  0.125490  0.929412  0.992157  0.988235  0.278431  0.000000   \n",
            "3     0.0  0.000000  0.019608  0.247059  0.772549  0.000000  0.000000   \n",
            "4     0.0  0.003922  0.658824  0.949020  0.109804  0.000000  0.000000   \n",
            "...   ...       ...       ...       ...       ...       ...       ...   \n",
            "2995  0.0  0.039216  0.650980  0.882353  0.992157  0.992157  0.992157   \n",
            "2996  0.0  0.321569  0.807843  0.992157  0.988235  0.988235  0.988235   \n",
            "2997  0.0  0.000000  0.000000  0.031373  0.513725  0.839216  0.984314   \n",
            "2998  0.0  0.000000  0.000000  0.529412  0.968627  0.988235  0.909804   \n",
            "2999  0.0  0.000000  0.000000  0.000000  0.105882  0.482353  0.917647   \n",
            "\n",
            "           7         8         9    ...  190       191       192       193  \\\n",
            "0     0.988235  0.988235  0.929412  ...  0.0  0.000000  0.000000  0.027451   \n",
            "1     0.000000  0.486275  0.992157  ...  0.0  0.000000  0.000000  0.000000   \n",
            "2     0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
            "3     0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
            "4     0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
            "...        ...       ...       ...  ...  ...       ...       ...       ...   \n",
            "2995  0.964706  0.419608  0.015686  ...  0.0  0.219608  0.882353  0.992157   \n",
            "2996  0.992157  0.988235  0.733333  ...  0.0  0.000000  0.027451  0.800000   \n",
            "2997  0.984314  0.984314  0.984314  ...  0.0  0.000000  0.282353  0.984314   \n",
            "2998  0.490196  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
            "2999  0.960784  0.992157  0.992157  ...  0.0  0.000000  0.341176  0.976471   \n",
            "\n",
            "           194       195       196       197       198       199  \n",
            "0     0.698039  0.988235  0.941176  0.278431  0.074510  0.109804  \n",
            "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.125490  \n",
            "2     0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  \n",
            "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.698039  \n",
            "4     0.000000  0.000000  0.000000  0.000000  0.113725  0.996078  \n",
            "...        ...       ...       ...       ...       ...       ...  \n",
            "2995  0.992157  0.992157  0.403922  0.000000  0.000000  0.000000  \n",
            "2996  0.992157  0.839216  0.000000  0.000000  0.000000  0.000000  \n",
            "2997  0.984314  0.984314  0.898039  0.270588  0.058824  0.000000  \n",
            "2998  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  \n",
            "2999  0.992157  0.666667  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[3000 rows x 200 columns]\n",
            "Training output label --> One hot encoded\n",
            "      0  1\n",
            "0     1  0\n",
            "1     0  1\n",
            "2     0  1\n",
            "3     0  1\n",
            "4     0  1\n",
            "...  .. ..\n",
            "2995  1  0\n",
            "2996  1  0\n",
            "2997  1  0\n",
            "2998  0  1\n",
            "2999  1  0\n",
            "\n",
            "[3000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading testing feature vector and also labels of testing dataset seperately, also making modifications required."
      ],
      "metadata": {
        "id": "X4xWtO4anooa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First colum of test dataset contains -1s so we need to drop the first column\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_test.csv', header=None)\n",
        "X_test = df_test.to_numpy()\n",
        "X_test = X_test[:,1:]\n",
        "X_test = X_test/255\n",
        "\n",
        "df_test1 = pd.read_csv('/content/drive/MyDrive/Toy Dataset/toy_dataset_test_labels.csv', header=None)\n",
        "Y_test = df_test1.to_numpy()\n",
        "\n",
        "print(\"Testing input data --> 300 images with 200 cols(features)\")\n",
        "print(pd.DataFrame(X_test))\n",
        "\n",
        "print(\"Testing output label --> class wise segregation\")\n",
        "print(pd.DataFrame(Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujwe12Jdneli",
        "outputId": "949e56db-fd4f-40d0-c3e3-e74a9ed6fe98"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing input data --> 300 images with 200 cols(features)\n",
            "     0         1         2         3         4         5         6    \\\n",
            "0    0.0  0.000000  0.000000  0.035294  0.992157  0.992157  0.458824   \n",
            "1    0.0  0.000000  0.000000  0.000000  0.007843  0.607843  0.988235   \n",
            "2    0.0  0.156863  0.898039  0.992157  0.992157  0.992157  0.278431   \n",
            "3    0.0  0.000000  0.000000  0.000000  0.145098  0.988235  0.992157   \n",
            "4    0.0  0.000000  0.000000  0.000000  0.043137  0.607843  0.913725   \n",
            "..   ...       ...       ...       ...       ...       ...       ...   \n",
            "295  0.0  0.000000  0.000000  0.000000  0.443137  1.000000  0.996078   \n",
            "296  0.0  0.000000  0.000000  0.019608  0.211765  0.533333  0.956863   \n",
            "297  0.0  0.000000  0.000000  0.000000  0.321569  0.992157  0.988235   \n",
            "298  0.0  0.000000  0.160784  0.517647  0.992157  0.996078  0.992157   \n",
            "299  0.0  0.000000  0.000000  0.000000  0.811765  0.996078  0.862745   \n",
            "\n",
            "          7         8         9    ...  190  191       192       193  \\\n",
            "0    0.000000  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "1    0.329412  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "2    0.000000  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "3    0.137255  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "4    0.988235  0.988235  0.988235  ...  0.0  0.0  0.000000  0.000000   \n",
            "..        ...       ...       ...  ...  ...  ...       ...       ...   \n",
            "295  0.996078  0.301961  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "296  0.996078  1.000000  0.694118  ...  0.0  0.0  0.254902  0.992157   \n",
            "297  0.717647  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "298  0.996078  0.992157  0.996078  ...  0.0  0.0  0.160784  0.992157   \n",
            "299  0.066667  0.000000  0.000000  ...  0.0  0.0  0.000000  0.000000   \n",
            "\n",
            "          194       195       196       197       198       199  \n",
            "0    0.000000  0.000000  0.000000  0.003922  0.435294  0.992157  \n",
            "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.945098  \n",
            "2    0.000000  0.000000  0.000000  0.000000  0.819608  0.992157  \n",
            "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.854902  \n",
            "4    0.000000  0.529412  0.992157  0.992157  0.815686  0.933333  \n",
            "..        ...       ...       ...       ...       ...       ...  \n",
            "295  0.662745  0.992157  0.992157  0.568627  0.003922  0.313725  \n",
            "296  0.992157  0.952941  0.243137  0.000000  0.000000  0.000000  \n",
            "297  0.000000  0.000000  0.000000  0.000000  0.000000  0.443137  \n",
            "298  0.988235  0.317647  0.000000  0.000000  0.000000  0.000000  \n",
            "299  0.000000  0.000000  0.000000  0.000000  0.000000  0.141176  \n",
            "\n",
            "[300 rows x 200 columns]\n",
            "Testing output label --> class wise segregation\n",
            "     0\n",
            "0    1\n",
            "1    1\n",
            "2    1\n",
            "3    1\n",
            "4    0\n",
            "..  ..\n",
            "295  0\n",
            "296  0\n",
            "297  1\n",
            "298  0\n",
            "299  1\n",
            "\n",
            "[300 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Setting Neural Network Parameters like number of epochs, batch_size, neurons in each layer(list is passed), learning rate type(for normal gradient descent, adaptive gradient descent), learning rate, activation type(sigmoid activation, tanh activation, RELU activation, Softmax Activation), loss function type(Cross Entropy loss Function, Mean Squared Error loss function), initial seed value(Taking default value as 87)."
      ],
      "metadata": {
        "id": "IDjjQfJ_nkkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to open params.txt here instead of manual data entering\n",
        "epochs = 500\n",
        "batch_size = 100\n",
        "layers = [100, 50, 20, 2]\n",
        "lr_type = 1\n",
        "lr = 2\n",
        "act_type = 0\n",
        "loss_type = 0\n",
        "seed_val = 22"
      ],
      "metadata": {
        "id": "1hCIzv3LnmvT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing random initialisation of neural nets weights matrix according to Xavier's Initialisation of weights."
      ],
      "metadata": {
        "id": "aQelHk51osAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training datavalues\n",
        "N = X_train.shape[0]\n",
        "\n",
        "weights = []\n",
        "\n",
        "np.random.seed(seed_val)\n",
        "\n",
        "# Initial number of neurons which is feature of dataset then after each iteration this will change to previous layers neurons\n",
        "m = X_train.shape[1]\n",
        "\n",
        "# Initialising weights\n",
        "for i in range(len(layers)):\n",
        "    n = layers[i]\n",
        "\n",
        "    # Initialisation of weights of each layer,\n",
        "    # Random data values from a normal distribution whose mean is 0 \n",
        "    # and std. deviation is 1 and output dimension is (m+1 cross n) [previous_layer_neurons+1  cross current_layer_neurons] \n",
        "    weights.append(np.float64(np.random.normal(0,1,(m+1,n))*np.sqrt(2/(m+n+1))))\n",
        "\n",
        "    # change the number of input neurons\n",
        "    m = n\n",
        "\n",
        "# Prints weight matrix of last layer\n",
        "print(\"Weight matrices at different levels of our Neural Network\")\n",
        "for i in range(len(layers)):\n",
        "    print(weights[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2lfcUsXo0TA",
        "outputId": "1a8000dc-5dd6-4ecc-9a7b-fb52f4495403"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight matrices at different levels of our Neural Network\n",
            "(201, 100)\n",
            "(101, 50)\n",
            "(51, 20)\n",
            "(21, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing Training Dataset into batches of batch_size."
      ],
      "metadata": {
        "id": "8_TfhRGKpU5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing data into batches\n",
        "training_batch = []\n",
        "# Adding a columns of ones to absorb bias in the training data\n",
        "\n",
        "print(\"Dividing the training dataset into batches for easy and efficient usage!!\")\n",
        "print(\"======================================================\")\n",
        "print(\"Adding a column of 1s in the dataset, just to absorb bias in the neural network\")\n",
        "X_train = np.concatenate((np.ones((X_train.shape[0],1)), X_train), axis=1)\n",
        "print(pd.DataFrame(X_train))\n",
        "\n",
        "number_batches = N//batch_size\n",
        "for i in range(number_batches):\n",
        "    # Adding a tupple of (X,Y) in batches\n",
        "    training_batch.append((X_train[i*batch_size:(i+1)*batch_size,:], Y_train[i*batch_size:(i+1)*batch_size,:]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4PZMPk7pkl2",
        "outputId": "9863fa6b-8019-423a-8443-59d74db76c7e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing the training dataset into batches for easy and efficient usage!!\n",
            "======================================================\n",
            "Adding a column of 1s in the dataset, just to absorb bias in the neural network\n",
            "      0    1         2         3         4         5         6         7    \\\n",
            "0     1.0  0.0  0.000000  0.000000  0.000000  0.188235  0.933333  0.988235   \n",
            "1     1.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2     1.0  0.0  0.125490  0.929412  0.992157  0.988235  0.278431  0.000000   \n",
            "3     1.0  0.0  0.000000  0.019608  0.247059  0.772549  0.000000  0.000000   \n",
            "4     1.0  0.0  0.003922  0.658824  0.949020  0.109804  0.000000  0.000000   \n",
            "...   ...  ...       ...       ...       ...       ...       ...       ...   \n",
            "2995  1.0  0.0  0.039216  0.650980  0.882353  0.992157  0.992157  0.992157   \n",
            "2996  1.0  0.0  0.321569  0.807843  0.992157  0.988235  0.988235  0.988235   \n",
            "2997  1.0  0.0  0.000000  0.000000  0.031373  0.513725  0.839216  0.984314   \n",
            "2998  1.0  0.0  0.000000  0.000000  0.529412  0.968627  0.988235  0.909804   \n",
            "2999  1.0  0.0  0.000000  0.000000  0.000000  0.105882  0.482353  0.917647   \n",
            "\n",
            "           8         9    ...  191       192       193       194       195  \\\n",
            "0     0.988235  0.988235  ...  0.0  0.000000  0.000000  0.027451  0.698039   \n",
            "1     0.000000  0.486275  ...  0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "2     0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "3     0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "4     0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "...        ...       ...  ...  ...       ...       ...       ...       ...   \n",
            "2995  0.964706  0.419608  ...  0.0  0.219608  0.882353  0.992157  0.992157   \n",
            "2996  0.992157  0.988235  ...  0.0  0.000000  0.027451  0.800000  0.992157   \n",
            "2997  0.984314  0.984314  ...  0.0  0.000000  0.282353  0.984314  0.984314   \n",
            "2998  0.490196  0.000000  ...  0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "2999  0.960784  0.992157  ...  0.0  0.000000  0.341176  0.976471  0.992157   \n",
            "\n",
            "           196       197       198       199       200  \n",
            "0     0.988235  0.941176  0.278431  0.074510  0.109804  \n",
            "1     0.000000  0.000000  0.000000  0.000000  0.125490  \n",
            "2     0.000000  0.000000  0.000000  0.000000  1.000000  \n",
            "3     0.000000  0.000000  0.000000  0.000000  0.698039  \n",
            "4     0.000000  0.000000  0.000000  0.113725  0.996078  \n",
            "...        ...       ...       ...       ...       ...  \n",
            "2995  0.992157  0.403922  0.000000  0.000000  0.000000  \n",
            "2996  0.839216  0.000000  0.000000  0.000000  0.000000  \n",
            "2997  0.984314  0.898039  0.270588  0.058824  0.000000  \n",
            "2998  0.000000  0.000000  0.000000  0.000000  1.000000  \n",
            "2999  0.666667  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[3000 rows x 201 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Training Our Neural Network According to given training Dataset"
      ],
      "metadata": {
        "id": "nXetEuJJs8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making loss array to store loss value after each epoch to compare loss values according to epoch\n",
        "tmp = lr\n",
        "loss = []\n",
        "\n",
        "print(\"Now training the model on the training dataset divided in batches\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # For adaptive gradient descent\n",
        "    if lr_type == 1:\n",
        "        lr = tmp/math.sqrt(epoch+1)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # Go over every batch and train my neural network\n",
        "    for batch, Y in training_batch:\n",
        "        values = []\n",
        "        values.append(batch)\n",
        "\n",
        "        # store original batch as this will get modified in next for loop\n",
        "        batch1 = batch\n",
        "        # Forward Propogation this batch so that it can be used in Backpropogation\n",
        "        for i in range(len(layers)-1):\n",
        "            batch = np.dot(batch, weights[i])\n",
        "            if act_type == 0:\n",
        "                batch = sigmoid(batch)\n",
        "            elif act_type == 1:\n",
        "                batch = tanh(batch)\n",
        "            else:\n",
        "                batch = relu(batch)\n",
        "            \n",
        "            # Concatenate with a columns of ones after output from each layer to absorb bias in the data\n",
        "            batch = np.concatenate((np.ones((batch_size, 1)),batch), axis = 1)\n",
        "\n",
        "            # catching this in values so that it can be used in backpropogation\n",
        "            values.append(batch)\n",
        "        \n",
        "        # forward propogation for last layer\n",
        "        batch = np.dot(batch, weights[i+1])\n",
        "\n",
        "        # Activation function for last layer\n",
        "        if loss_type == 0:\n",
        "            output = softmax(batch)\n",
        "        else:\n",
        "            output = tanh(batch_size)\n",
        "\n",
        "        # Final output of this neural network for this batch without tweeking weights wrong output most probably\n",
        "        values.append(output)\n",
        "\n",
        "        # Calculating derivative of loss function for backpropogation of neural net\n",
        "        if loss_type == 0:\n",
        "            der = (output-Y)/Y.shape[0]\n",
        "\n",
        "        else:\n",
        "            if act_type == 0:\n",
        "                der = (output-Y)*derivative_sigmoid(output)/Y.shape[0]\n",
        "            elif act_type:\n",
        "                der = (output-Y)*derivative_tanh(output)/Y.shape[0]\n",
        "            else:\n",
        "                der = (output-Y)*derivative_relu(output)/Y.shape[0]\n",
        "        \n",
        "        # Back propogating error \n",
        "        for i in range(len(layers)-1,-1,-1):\n",
        "            weight_copy = weights[i].copy()\n",
        "            w = np.dot(values[i].T, der)\n",
        "\n",
        "            weights[i] -= lr*w\n",
        "\n",
        "            if act_type == 0:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_sigmoid(values[i])\n",
        "            elif act_type == 1:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_tanh(values[i])\n",
        "            else:\n",
        "                der = np.dot(der, weight_copy.T)*derivative_relu(values[i])\n",
        "            \n",
        "            der = np.delete(der, 0, axis = 1)\n",
        "\n",
        "        # Calculating predicted output of this batch to calculate loss value of this batch\n",
        "        output = batch1\n",
        "        for i in range(len(layers)-1):\n",
        "            output = np.dot(output, weights[i])\n",
        "            if act_type == 0:\n",
        "                output = sigmoid(output)\n",
        "            elif act_type == 1:\n",
        "                output = tanh(output)\n",
        "            else:\n",
        "                output = relu(output)\n",
        "\n",
        "            output = np.concatenate((np.ones((output.shape[0], 1)), output), axis = 1)\n",
        "        \n",
        "        # Forward propogating for output layer\n",
        "        output = np.dot(output, weights[i+1])\n",
        "        # Activation function for last layer\n",
        "        if loss_type == 0:\n",
        "            output = softmax(output)\n",
        "        else:\n",
        "            output = tanh(output)\n",
        "\n",
        "        # According to loss type calculate loss value of this batch\n",
        "        if loss_type == 0:\n",
        "            total_loss += CE(Y, output)\n",
        "        else:\n",
        "            total_loss += MSE(Y, output)\n",
        "    # append to this epoch's loss value\n",
        "    loss.append(total_loss)\n",
        "\n",
        "print(\"Graphical representation of Training Loss vs number of epochs -->\")\n",
        "plt.plot([(epoch+1) for epoch in range(epochs)], loss)\n",
        "plt.show() \n",
        "        \n",
        "# Predicting output for testing dataset\n",
        "for i in range(len(layers)-1):\n",
        "    X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "    X_test = np.dot(X_test, weights[i])\n",
        "    \n",
        "    if act_type == 0:\n",
        "        X_test = sigmoid(X_test)\n",
        "    elif act_type == 1:\n",
        "        X_test = tanh(X_test)\n",
        "    else:\n",
        "        X_test = relu(X_test)\n",
        "\n",
        "X_test = np.concatenate((np.ones((X_test.shape[0],1)), X_test), axis = 1)\n",
        "X_test = np.dot(X_test, weights[i+1])\n",
        "\n",
        "if loss_type == 0:\n",
        "    output = softmax(X_test)\n",
        "else:\n",
        "    output = tanh(X_test)\n",
        "pred = np.argmax(output, axis=1)\n",
        "\n",
        "correct = 0\n",
        "total = pred.shape[0]\n",
        "\n",
        "for i in range(total):\n",
        "    if pred[i] == Y_test[i]:\n",
        "        correct+=1\n",
        "\n",
        "print(\"Accuracy on testing dataset -->\")\n",
        "print((correct/total)*100)        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "axQg8rkGtEPY",
        "outputId": "4c1f1f40-7426-407f-a275-0a1641115a77"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now training the model on the training dataset divided in batches\n",
            "Graphical representation of Training Loss vs number of epochs -->\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.0\n"
          ]
        }
      ]
    }
  ]
}